"""
PoC: Fine-tune Embedding for Vulnerability-Aware Representation

Goal: Train an embedding model that encodes vulnerability information,
potentially replacing the separate Ridge regression model.

Approach: Contrastive learning
- Positive pairs: facts with similar vulnerability
- Negative pairs: facts with different vulnerability

Evaluation: Compare with baseline (BGE-M3 + Ridge)
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))


@dataclass
class VulnerabilityPair:
    """A pair of facts for contrastive learning."""
    text_a: str
    text_b: str
    similarity: float  # 1.0 = same vulnerability, 0.0 = opposite


def load_training_data() -> Tuple[List[Dict], np.ndarray]:
    """Load facts with vulnerability labels."""
    # Load from Phase 1 training
    with open('experiments/models/vulnerability_training_data.json') as f:
        data = json.load(f)
    return data['samples'], np.array(data['y'])


def create_contrastive_pairs(
    samples: List[Dict],
    vulnerabilities: np.ndarray,
    n_pairs: int = 500,
) -> List[VulnerabilityPair]:
    """
    Create contrastive pairs for training.

    Strategy:
    - Similar vulnerability (|v1 - v2| < 0.1) → similarity = 1.0
    - Different vulnerability (|v1 - v2| > 0.3) → similarity = 0.0
    """
    pairs = []
    n = len(samples)

    np.random.seed(42)

    for _ in range(n_pairs):
        i, j = np.random.choice(n, 2, replace=False)

        # Create text with context
        text_a = f"Turn {samples[i]['turn_id']}/50, Type: {samples[i]['type']}, Content: {samples[i]['content']}"
        text_b = f"Turn {samples[j]['turn_id']}/50, Type: {samples[j]['type']}, Content: {samples[j]['content']}"

        # Compute similarity based on vulnerability difference
        v_diff = abs(vulnerabilities[i] - vulnerabilities[j])

        if v_diff < 0.1:
            similarity = 1.0  # Very similar vulnerability
        elif v_diff > 0.3:
            similarity = 0.0  # Very different vulnerability
        else:
            similarity = 1.0 - v_diff  # Linear interpolation

        pairs.append(VulnerabilityPair(
            text_a=text_a,
            text_b=text_b,
            similarity=similarity,
        ))

    return pairs


def prepare_sentence_transformers_data(pairs: List[VulnerabilityPair]) -> Dict:
    """Prepare data in sentence-transformers format."""
    return {
        'texts_a': [p.text_a for p in pairs],
        'texts_b': [p.text_b for p in pairs],
        'labels': [p.similarity for p in pairs],
    }


# ============================================================
# Option 1: Using sentence-transformers (Recommended for PoC)
# ============================================================

def finetune_with_sentence_transformers(
    pairs: List[VulnerabilityPair],
    base_model: str = 'BAAI/bge-small-en-v1.5',  # Smaller for PoC
    output_path: str = 'experiments/models/vuln_embedding',
    epochs: int = 3,
):
    """
    Fine-tune using sentence-transformers library.

    This is the easiest approach for PoC.
    """
    try:
        from sentence_transformers import SentenceTransformer, InputExample, losses
        from torch.utils.data import DataLoader
    except ImportError:
        print("Please install: pip install sentence-transformers")
        return None

    print(f"Loading base model: {base_model}")
    model = SentenceTransformer(base_model)

    # Prepare training data
    train_examples = [
        InputExample(texts=[p.text_a, p.text_b], label=p.similarity)
        for p in pairs
    ]

    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    train_loss = losses.CosineSimilarityLoss(model)

    print(f"Training for {epochs} epochs on {len(pairs)} pairs...")

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=100,
        output_path=output_path,
        show_progress_bar=True,
    )

    print(f"Model saved to: {output_path}")
    return model


# ============================================================
# Option 2: Using transformers + LoRA (For Qwen)
# ============================================================

def finetune_qwen_lora(
    pairs: List[VulnerabilityPair],
    base_model: str = 'Qwen/Qwen2-0.5B',  # Small Qwen for PoC
    output_path: str = 'experiments/models/vuln_qwen_lora',
):
    """
    Fine-tune Qwen with LoRA for vulnerability-aware embedding.

    This requires more setup but can leverage Qwen's capabilities.
    """
    try:
        import torch
        from transformers import AutoTokenizer, AutoModel
        from peft import LoraConfig, get_peft_model, TaskType
    except ImportError:
        print("Please install: pip install transformers peft torch")
        return None

    print(f"Loading Qwen model: {base_model}")
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModel.from_pretrained(base_model, trust_remote_code=True)

    # Configure LoRA
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )

    model = get_peft_model(model, lora_config)
    print(f"Trainable params: {model.print_trainable_parameters()}")

    # Training loop would go here...
    # For PoC, we'll just show the setup

    print("LoRA setup complete. Full training requires GPU and more code.")
    return model, tokenizer


# ============================================================
# Evaluation: Compare fine-tuned vs baseline
# ============================================================

def evaluate_embedding_quality(
    model,
    samples: List[Dict],
    vulnerabilities: np.ndarray,
) -> Dict:
    """
    Evaluate how well the embedding predicts vulnerability.

    Method: Train a simple linear probe on embeddings.
    """
    from sklearn.linear_model import Ridge
    from sklearn.model_selection import cross_val_score
    from sklearn.preprocessing import StandardScaler

    # Get embeddings
    texts = [
        f"Turn {s['turn_id']}/50, Type: {s['type']}, Content: {s['content']}"
        for s in samples
    ]

    if hasattr(model, 'encode'):
        embeddings = model.encode(texts, show_progress_bar=True)
    else:
        # For transformers model
        embeddings = []
        for text in texts:
            inputs = model.tokenizer(text, return_tensors='pt', truncation=True)
            with torch.no_grad():
                outputs = model.model(**inputs)
            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
        embeddings = np.array(embeddings)

    # Linear probe
    scaler = StandardScaler()
    X = scaler.fit_transform(embeddings)

    # Cross-validation
    probe = Ridge(alpha=1.0)
    scores = cross_val_score(probe, X, vulnerabilities, cv=5, scoring='r2')

    return {
        'mean_r2': float(np.mean(scores)),
        'std_r2': float(np.std(scores)),
        'embedding_dim': embeddings.shape[1],
    }


def compare_with_baseline(
    finetuned_model,
    samples: List[Dict],
    vulnerabilities: np.ndarray,
) -> Dict:
    """Compare fine-tuned model with baseline (BGE + Ridge on handcrafted features)."""

    # Baseline: Load from Phase 1
    with open('experiments/models/vulnerability_training_data.json') as f:
        baseline_data = json.load(f)

    X_baseline = np.array(baseline_data['X'])
    y_pred_baseline = np.array(baseline_data['y_pred'])

    from sklearn.metrics import r2_score, mean_squared_error

    baseline_r2 = r2_score(vulnerabilities, y_pred_baseline)
    baseline_mse = mean_squared_error(vulnerabilities, y_pred_baseline)

    # Fine-tuned
    finetuned_results = evaluate_embedding_quality(finetuned_model, samples, vulnerabilities)

    comparison = {
        'baseline': {
            'method': 'BGE-M3 (frozen) + Handcrafted Features + Ridge',
            'r2': baseline_r2,
            'mse': baseline_mse,
            'features': X_baseline.shape[1],
        },
        'finetuned': {
            'method': 'Vulnerability-Tuned Embedding + Linear Probe',
            'r2': finetuned_results['mean_r2'],
            'r2_std': finetuned_results['std_r2'],
            'embedding_dim': finetuned_results['embedding_dim'],
        },
        'winner': 'finetuned' if finetuned_results['mean_r2'] > baseline_r2 else 'baseline',
        'improvement': finetuned_results['mean_r2'] - baseline_r2,
    }

    return comparison


def main():
    print("=" * 70)
    print("VULNERABILITY EMBEDDING FINE-TUNING PoC")
    print("=" * 70)

    # Load data
    print("\n[1] Loading training data...")
    samples, vulnerabilities = load_training_data()
    print(f"    Loaded {len(samples)} samples")
    print(f"    Vulnerability range: [{vulnerabilities.min():.3f}, {vulnerabilities.max():.3f}]")

    # Create contrastive pairs
    print("\n[2] Creating contrastive pairs...")
    pairs = create_contrastive_pairs(samples, vulnerabilities, n_pairs=500)

    n_similar = sum(1 for p in pairs if p.similarity > 0.8)
    n_different = sum(1 for p in pairs if p.similarity < 0.2)
    print(f"    Created {len(pairs)} pairs")
    print(f"    Similar vulnerability pairs: {n_similar}")
    print(f"    Different vulnerability pairs: {n_different}")

    # Save pairs for inspection
    pairs_path = Path('experiments/data/contrastive_pairs.json')
    with open(pairs_path, 'w') as f:
        json.dump([
            {'text_a': p.text_a, 'text_b': p.text_b, 'similarity': p.similarity}
            for p in pairs[:20]  # Sample
        ], f, indent=2)
    print(f"    Sample pairs saved to: {pairs_path}")

    # Fine-tuning (requires GPU, skip if not available)
    print("\n[3] Fine-tuning options:")
    print("""
    Option A: sentence-transformers (Recommended for PoC)
        model = finetune_with_sentence_transformers(pairs)

    Option B: Qwen + LoRA (For production)
        model = finetune_qwen_lora(pairs)

    To run fine-tuning, uncomment the desired option below.
    Requires: GPU, ~10 minutes for PoC
    """)

    # Run fine-tuning
    print("\n[4] Running fine-tuning...")
    model = finetune_with_sentence_transformers(pairs)

    if model:
        print("\n[5] Evaluating fine-tuned model...")
        comparison = compare_with_baseline(model, samples, vulnerabilities)
        print(f"\nComparison: {json.dumps(comparison, indent=2)}")

        # Save results
        results_path = Path('experiments/results/finetune_comparison.json')
        with open(results_path, 'w') as f:
            json.dump(comparison, f, indent=2)
        print(f"Results saved to: {results_path}")

    print("\n" + "=" * 70)
    print("EXPECTED OUTCOME")
    print("=" * 70)
    print("""
    ┌─────────────────────────────────────┬────────┬─────────────────┐
    │ Method                              │ R²     │ Decision        │
    ├─────────────────────────────────────┼────────┼─────────────────┤
    │ Baseline: BGE + Ridge               │ 0.200  │ Current         │
    │ Fine-tuned Embedding + Linear       │ 0.25+? │ If better, use  │
    └─────────────────────────────────────┴────────┴─────────────────┘

    If fine-tuned R² > 0.200:
        → Replace Ridge with fine-tuned embedding
        → Bonus: clustering also improves (same embedding)

    If fine-tuned R² <= 0.200:
        → Keep baseline (BGE + Ridge)
        → Fine-tuning didn't help (data too small?)
    """)

    # Save training config for reproducibility
    config_path = Path('experiments/models/finetune_config.json')
    config = {
        'n_samples': len(samples),
        'n_pairs': len(pairs),
        'base_model_options': [
            'BAAI/bge-small-en-v1.5',  # 33M params
            'BAAI/bge-base-en-v1.5',   # 110M params
            'Qwen/Qwen2-0.5B',         # 500M params
        ],
        'training_epochs': 3,
        'batch_size': 16,
        'loss': 'CosineSimilarityLoss',
    }
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"\nConfig saved to: {config_path}")


if __name__ == '__main__':
    main()
