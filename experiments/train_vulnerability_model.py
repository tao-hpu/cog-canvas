"""
Phase 1: Train Vulnerability Model using CogCanvas Recall Results

Key Innovation:
Instead of binary lost/retained labels based on position threshold,
we use ACTUAL retrieval performance as the ground truth:
- label = 0.0 if passed (successfully recalled)
- label = 1.0 - fuzzy_score/100 if failed (vulnerability = retrieval difficulty)

This gives us a continuous vulnerability signal that reflects real-world
information loss, not just position-based heuristics.
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict
import re
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from cogcanvas.vage_learned import VulnerabilityScorer


def load_recall_results(path: str = 'experiments/results/cogcanvas_50_topk5.json') -> Dict:
    """Load CogCanvas retrieval results."""
    with open(path) as f:
        return json.load(f)


def load_facts_with_features(path: str = 'experiments/data/facts_with_clusters.json') -> List[Dict]:
    """Load facts with cluster IDs and other features."""
    with open(path) as f:
        data = json.load(f)
    return data['facts']


def has_numbers(text: str) -> bool:
    """Check if text contains numbers."""
    return bool(re.search(r'\d+', text))


def has_named_entities(text: str) -> bool:
    """Simple heuristic: capitalized words that aren't sentence starters."""
    words = text.split()
    for i, word in enumerate(words):
        if i > 0 and word and word[0].isupper() and not word.isupper():
            prev = words[i-1] if i > 0 else ""
            if not prev.endswith('.'):
                return True
    return False


def extract_training_data(
    recall_results: Dict,
    facts_with_features: List[Dict],
    total_turns: int = 50,
) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """
    Extract training data from recall results.

    Labels:
    - passed=True: vulnerability = 0.0 (perfectly retained)
    - passed=False: vulnerability = 1.0 - fuzzy_score/100

    Features:
    - position_ratio: turn_id / total_turns
    - content_length_norm: len(content) / 100
    - quote_length_norm: len(quote) / 500
    - has_numbers: binary
    - has_named_entities: binary
    - type_onehot: 6 dimensions
    - cluster_id_onehot: 8 dimensions

    Returns:
        X: Feature matrix (n_samples, n_features)
        y: Vulnerability labels (n_samples,)
        samples: List of sample dicts for analysis
    """
    # Build fact_id -> fact features mapping
    fact_features = {f['id']: f for f in facts_with_features}

    # Fact types and clusters for one-hot encoding
    fact_types = ['decision', 'key_fact', 'reminder', 'insight', 'todo', 'unknown']
    n_clusters = 8

    X = []
    y = []
    samples = []

    for conv in recall_results['conversations']:
        for fact in conv['facts']:
            fact_id = fact['id']

            # Get corresponding features
            if fact_id not in fact_features:
                continue

            feat = fact_features[fact_id]

            # Compute vulnerability label
            if fact['passed']:
                vulnerability = 0.0
            else:
                vulnerability = 1.0 - fact['fuzzy_score'] / 100.0

            # Extract features
            content = feat.get('content', '')
            quote = feat.get('quote', '')
            turn_id = feat.get('turn_id', 0)
            fact_type = feat.get('type', 'unknown')
            cluster_id = feat.get('cluster_id', 0)

            # Basic features
            position_ratio = turn_id / total_turns
            content_length_norm = len(content) / 100
            quote_length_norm = len(quote) / 500
            has_nums = float(has_numbers(content + quote))
            has_entities = float(has_named_entities(quote))

            # Type one-hot
            type_onehot = [1.0 if fact_type == t else 0.0 for t in fact_types]

            # Cluster one-hot
            cluster_onehot = [1.0 if cluster_id == c else 0.0 for c in range(n_clusters)]

            # Combine features
            features = [
                position_ratio,
                content_length_norm,
                quote_length_norm,
                has_nums,
                has_entities,
            ] + type_onehot + cluster_onehot

            X.append(features)
            y.append(vulnerability)

            samples.append({
                'fact_id': fact_id,
                'content': content,
                'type': fact_type,
                'cluster_id': cluster_id,
                'turn_id': turn_id,
                'passed': fact['passed'],
                'fuzzy_score': fact['fuzzy_score'],
                'vulnerability': vulnerability,
            })

    return np.array(X), np.array(y), samples


def train_model(X: np.ndarray, y: np.ndarray) -> VulnerabilityScorer:
    """Train vulnerability model with regression target."""
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import StandardScaler

    print(f"\nTraining on {len(y)} samples")
    print(f"Label range: [{y.min():.3f}, {y.max():.3f}]")
    print(f"Mean vulnerability: {y.mean():.3f}")
    print(f"Std vulnerability: {y.std():.3f}")

    # Check class balance (binary)
    n_failed = (y > 0).sum()
    n_passed = (y == 0).sum()
    print(f"\nClass distribution:")
    print(f"  Passed (vuln=0): {n_passed} ({n_passed/len(y)*100:.1f}%)")
    print(f"  Failed (vuln>0): {n_failed} ({n_failed/len(y)*100:.1f}%)")

    # Create and fit scorer
    scorer = VulnerabilityScorer()
    scorer.scaler = StandardScaler()
    X_scaled = scorer.scaler.fit_transform(X)

    # Use Ridge regression for continuous target
    scorer.model = Ridge(alpha=1.0)
    scorer.model.fit(X_scaled, y)
    scorer.is_fitted = True

    # Feature names for analysis
    feature_names = [
        'position_ratio', 'content_length', 'quote_length',
        'has_numbers', 'has_entities',
        'type_decision', 'type_key_fact', 'type_reminder',
        'type_insight', 'type_todo', 'type_unknown',
        'cluster_0', 'cluster_1', 'cluster_2', 'cluster_3',
        'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7',
    ]

    print("\n" + "=" * 50)
    print("MODEL COEFFICIENTS")
    print("=" * 50)

    coefs = list(zip(feature_names, scorer.model.coef_))
    coefs.sort(key=lambda x: abs(x[1]), reverse=True)

    for name, coef in coefs:
        direction = "↑" if coef > 0 else "↓" if coef < 0 else "="
        print(f"  {name:20s}: {coef:+.4f} {direction}")

    return scorer


def evaluate_model(scorer: VulnerabilityScorer, X: np.ndarray, y: np.ndarray, samples: List[Dict]):
    """Evaluate model performance."""
    from sklearn.metrics import mean_squared_error, r2_score

    X_scaled = scorer.scaler.transform(X)
    y_pred = scorer.model.predict(X_scaled)

    # Clip predictions to [0, 1]
    y_pred = np.clip(y_pred, 0, 1)

    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)

    print("\n" + "=" * 50)
    print("MODEL EVALUATION")
    print("=" * 50)
    print(f"MSE: {mse:.4f}")
    print(f"R²: {r2:.4f}")

    # Binary classification metrics (passed vs failed)
    y_binary = (y > 0).astype(int)
    y_pred_binary = (y_pred > 0.1).astype(int)  # threshold at 0.1

    from sklearn.metrics import precision_score, recall_score, f1_score

    precision = precision_score(y_binary, y_pred_binary, zero_division=0)
    recall = recall_score(y_binary, y_pred_binary, zero_division=0)
    f1 = f1_score(y_binary, y_pred_binary, zero_division=0)

    print(f"\nBinary Classification (threshold=0.1):")
    print(f"  Precision: {precision:.3f}")
    print(f"  Recall: {recall:.3f}")
    print(f"  F1: {f1:.3f}")

    # Analyze errors
    errors = y_pred - y
    worst_idx = np.argsort(np.abs(errors))[-5:][::-1]

    print("\n" + "=" * 50)
    print("WORST PREDICTIONS")
    print("=" * 50)

    for idx in worst_idx:
        s = samples[idx]
        print(f"\n  Fact: {s['content'][:50]}...")
        print(f"  Type: {s['type']}, Cluster: {s['cluster_id']}, Turn: {s['turn_id']}")
        print(f"  Actual vuln: {y[idx]:.3f}, Predicted: {y_pred[idx]:.3f}")
        print(f"  Error: {errors[idx]:+.3f}")

    return y_pred


def analyze_cluster_vulnerability(samples: List[Dict], y: np.ndarray):
    """Analyze vulnerability by cluster."""
    from collections import defaultdict

    cluster_vulns = defaultdict(list)
    for i, s in enumerate(samples):
        cluster_vulns[s['cluster_id']].append(y[i])

    print("\n" + "=" * 50)
    print("CLUSTER VULNERABILITY ANALYSIS")
    print("=" * 50)

    print(f"\n{'Cluster':<10} {'N':>6} {'Mean Vuln':>12} {'Std':>10} {'Interpretation':<25}")
    print("-" * 65)

    for cid in sorted(cluster_vulns.keys()):
        vulns = cluster_vulns[cid]
        mean_v = np.mean(vulns)
        std_v = np.std(vulns)

        if mean_v > 0.1:
            interp = "⚠️  HIGH RISK"
        elif mean_v > 0.03:
            interp = "MODERATE"
        else:
            interp = "LOW RISK"

        print(f"{cid:<10} {len(vulns):>6} {mean_v:>11.3f} {std_v:>10.3f} {interp:<25}")


def main():
    print("=" * 70)
    print("PHASE 1: TRAIN VULNERABILITY MODEL")
    print("=" * 70)

    # Load data
    print("\n[1] Loading data...")
    try:
        recall_results = load_recall_results()
        facts_with_features = load_facts_with_features()
        print(f"    Loaded recall results: {len(recall_results['conversations'])} conversations")
        print(f"    Loaded {len(facts_with_features)} facts with features")
    except FileNotFoundError as e:
        print(f"    ❌ Error: {e}")
        return

    # Extract training data
    print("\n[2] Extracting training data...")
    X, y, samples = extract_training_data(recall_results, facts_with_features)
    print(f"    Feature matrix: {X.shape}")
    print(f"    Label vector: {y.shape}")

    # Train model
    print("\n[3] Training vulnerability model...")
    scorer = train_model(X, y)

    # Evaluate
    print("\n[4] Evaluating model...")
    y_pred = evaluate_model(scorer, X, y, samples)

    # Cluster analysis
    print("\n[5] Analyzing cluster vulnerabilities...")
    analyze_cluster_vulnerability(samples, y)

    # Save model
    print("\n[6] Saving model...")
    output_dir = Path('experiments/models')
    output_dir.mkdir(exist_ok=True)

    model_path = output_dir / 'vulnerability_scorer.pkl'
    scorer.save(str(model_path))
    print(f"    Saved to: {model_path}")

    # Save training data for future use
    training_data_path = output_dir / 'vulnerability_training_data.json'
    with open(training_data_path, 'w') as f:
        json.dump({
            'samples': samples,
            'X': X.tolist(),
            'y': y.tolist(),
            'y_pred': y_pred.tolist(),
        }, f, indent=2)
    print(f"    Training data saved to: {training_data_path}")

    # Summary
    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print("=" * 70)
    print("""
Key findings:
1. Model trained on ACTUAL retrieval success as labels
2. Features include position, content, type, AND cluster

Use in VAGE:
    from cogcanvas.vage_learned import VulnerabilityScorer
    scorer = VulnerabilityScorer()
    scorer.load('experiments/models/vulnerability_scorer.pkl')
    vuln = scorer.score_object(turn_id, total_turns, content, obj_type)
    """)


if __name__ == '__main__':
    main()
